---
title: "Monte Carlo Methods Midterm"
author: "Kylie Taylor"
date: "3/28/2019"
output: pdf_document
---

Suppose observations $y_{ij}$ for i =1,...,n and for j = 1,...,m arise given ($v_i$) accroding to 

$log(y_{ij}) = \mu + log(\frac{v_i}{v_i + x_{ij}}) + \sigma \epsilon{ij}$

where ($\epsilon{ij}$) are iid standard normal 

$\epsilon{ij} \sim iid N(0,1) \propto e^{-\frac{1}{2}x^2}$

The $log(v_i)$ are random parameters and i.i.d. from a normal distribution with mean $\nu$ and variance $\phi^2$

$f(log(v_i)) \sim iid N(\nu, \phi^2) \propto \frac{1}{\sqrt{2\pi\phi^2}} e^{-\frac{(logv_i-\nu)^2}{2\phi^2}}$

The parameters of the model are $\theta = (\mu, \sigma, \nu, \phi)$.


The prior for $\mu$ is normal with mean 0 and variance $\xi^2$.

$f(\mu) \sim N(0, \xi^2) =  \frac{1}{\sqrt{2\pi\xi^2}} e^{-\frac{\mu^2}{2\xi^2}}$


The prior to $\nu$ is normal with mean 0 and variance $\psi^2$.

$f(\nu) \sim N(0, \psi^2) =  \frac{1}{\sqrt{2\pi\psi^2}} e^{-\frac{\nu^2}{2\psi^2}}$


The prior for $\eta=\frac{1}{\phi^2}$ is Gamma with parameters (c,d).

$f(\eta)=\frac{1}{\phi^2} \sim Ga(c, d) \propto \eta^{c-1} e^{-d\eta}$

The prior for $\lambda = \frac{1}{\sigma^2}$ is Gamma with parameters (a,b).

$f(\lambda) = \frac{1}{\sigma^2} \sim Ga(a, b) \propto \lambda^{a-1} e^{-b\lambda}$


The data are for n = 86 and m = 7 for each $i$, it is that $x_{ij} = j$.

The prior settings, i.e. (a, b, $\psi$, $\xi$, c, d)


```{r, include=FALSE}
Data <- read.table("~/Downloads/mid.dat.txt", quote="\"")
```


For this midterm, I will do the following steps:

1) Download and use the data, where I am given $x_{ij}$, i = 1,...,86 and j = 1,...,7

2) Pick a fixed $(\mu, \sigma, \nu, \phi)$ and generate $v_i = (v_1), ...,(v_{86})$ from $logN(\nu, \phi^2)$

3) Generate $log(y_{ij})$ from $N(\mu + log(\frac{v_i}{v_i + x_{ij}}), \sigma^2)$

4) Code the Markov Chain of the conditional densities $f(\mu|...) , f(\nu|...), f(\eta|...), f(\lambda|...)$ and $f(v_i|...)$

5) Report the output of the chain.

6) Find an estimate for $v_{87}$


#1) Data


The first step is to make sure the data is downloaded correctly. Since I do not know any information about the seven variables in this dataset, I cannot say that the summary statistics reveal anything alarming. 


```{r, echo=FALSE}
pander::pander(summary(Data))
```


The summary statistics of the 7 variables, V2-V8 (V1 is a count), reveal that every observation is bounded between 0 and 1.




#2) Generating $v_i$ from fixed $(\mu, \sigma, \nu, \phi)$


In order to generate samples $(v_1), ...,(v_{86})$ from $v_i \sim logN(\nu, \phi^2)$ I need to specify my parameters $(\mu, \sigma, \nu, \phi)$. I will specifiy these parameters by sampling from their joint densities, which are written above.

I will generate an estimate for $\mu$ by randomly generating 1,000,000 observations from $f(\mu) \sim N(0, \xi^2)$, then taking the average of all those samples. In order to do this, I must pick a $\xi^2$ value that I deem appropriate, which I chose to be 0.1. This makes the estimate for $\mu$ = 0.00014.


I will generate an estimate for $\nu$ using the same process as I did for $\mu$. I will generate 1,000,000 random samples from $f(\nu) \sim N(0, \psi^2)$, then take the average of all samples and use that as my fixed $\nu$. Since $\psi^2$ is not already specified, I will chose $\psi^2$ = 0.05. This makes the estimate of $\nu$=0.000024.


Following the same steps, I will generate an estimate for $\sigma^2$. Recall, $f(\lambda) = \frac{1}{\sigma^2} \sim Ga(a, b)$. This means I will be randomly sampling $f(\lambda)$, then solving for $\sigma^2$. In order to do this, I must chose an $(a,b)$ so I can generate 1,000,000 samples from $f(\lambda)$. I pick $a$ = 2 and $b$ = 1. This gave me an average estimate of $\lambda$ = 2.0. This makes $\sigma = \sqrt{\frac{1}{\lambda^2}} = \sqrt{0.25}$, or $\sigma^2$ = 0.25.


Lastly, I will pick a $\phi^2$ by randomly sampling from $f(\eta) = \frac{1}{\phi^2} \sim Ga(c, d)$. The $(c,d)$ that I chose are $c$ = 3, and $d$ = 2. This gives me an estimate for $\eta$ = 6.0 averaged over 1,000,000 samples. This means that $\phi = \sqrt{\frac{1}{\eta^2}} = \sqrt{0.02778}$, or that $\phi^2 = 0.02778$

```{r, include=FALSE}
set.seed(10000)
MU <- rnorm(1000000, 0, 0.1)
mean(MU)

NU <- rnorm(1000000, 0, 0.05)
mean(NU)

LAMBDA <- rgamma(1000000, shape = 2 , scale = 1)
mean(LAMBDA)

ETA <- rgamma(1000000, shape =3 , scale = 2)
mean(ETA)
```


Stated above, $log(v_i) \sim N(\nu, \phi^2)$ was given, this means that the distribution of $v_i$ is a log-normal, since it is simply $e^{log(v_i)}$. This is why I will now generate estimates for $v_i = (v_1), ...,(v_{86})$ from $logN(\nu, \phi^2)$ using the fixed parameters that I generated above. 
I will sample from $logN(\nu, \phi^2)$ 10,000 times, then take the average $v_i$ across all the generated samples and use that to continue my analysis. This leaves me with a 86 x 1 matrix with each entry as the bootstrapped sample for $v_i$ for every $i=1,..,86$.


```{r, include=FALSE}
set.seed(88888)
library(mosaic)
V.est = do(10000)*{
  V <- matrix(rlnorm(86, 0.000024, 0.02778), nrow = 86, ncol = 1)
}
```

```{r, include=FALSE}
v <- as.matrix(colMeans(V.est))
```




#3) Generating $log(y_{ij})$

I will now generate $log(y_{ij})$ from $N(\mu + log(\frac{v_i}{v_i + x_{ij}}), \sigma^2)$. I will be using the fixed parameters I determined earlier, the estimates for $v_i$, an 86x1 matrix, and the data, $x_{ij}$, and 86x7 matrix. I will sample  $N(\mu + log(\frac{v_i}{v_i + x_{ij}}), \sigma^2)$ 100,000 times, then take the average of all entries to have the final 86x7 matrix of $log(y_{ij})$ that I will use for implementing the Markov Chain and further analysis.

```{r, include=FALSE}
set.seed(888)
logY <- as.matrix(Data[,-1])
```




#4) Code Markov Chain

Now that I have my generated $log(y_{ij})$'s, I now must code the Markov chain of the conditional posterior denisties, $f(\mu|...) ,f(\nu|...),f(\eta|...),f(\lambda|...)$ and $f(v_i|...)$. My goal is to have Monte Carlo estimates for all of my parameters and for each $v_i$, that I can use to fit $log(y_{ij})$ and ultimately determine a value for $v_{87}$.

In order to do this, I must use a target density, which is $f(\theta)$, where $\theta = (y_{ij}, v_i, \mu, \nu, \lambda, \eta)$. This means that there will be 6 conditional densities. 

We were given the conditional density for $log(y_{ij})$ in the beginning, as 

$f(log(y_{ij}) | v_i, \mu, \nu, \lambda, \eta ) \sim N(\mu + log(\frac{v_i}{v_i + x_{ij}}), \sigma^2)$.

This will be used to solve for other conditional posterior densities.



To begin, I will define the joint density,

$f(y_{ij},v_i,\mu,\nu,\lambda,\eta)=f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)*f(v_{i}|\mu,\nu,\lambda,\eta)*f(\mu)*f(\nu)*f(\lambda)*f(\eta)$



The first conditional posterior density I will find is 

$f(\lambda|y_{ij},v_i,\mu,\nu,\eta) \propto f(\lambda)\prod_{i,j}f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)$

since $\lambda = \frac{1}{\sigma^2}$, the only other density where $\lambda$ shows up is in $f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)$. This makes

$f(\lambda|y_{ij},v_i,\mu,\nu,\eta) \propto \lambda^{a-1}e^{-b\lambda} \prod_{i,j}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}{2\sigma^2}}$


$\propto \lambda^{a-1}e^{-b\lambda} \lambda^{\frac{nm}{2}} e^{-\frac{\lambda}{2}\sum_{i,j}(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}$


$\propto \lambda^{a+\frac{nm}{2}-1} e^{-b\lambda-\frac{\lambda}{2}\sum_{i,j}(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}$


Therefore $f(\lambda|y_{ij},v_i,\mu,\nu,\eta) \sim Ga(a+\frac{nm}{2}, b+ \frac{1}{2}\sum_{i,j}(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}})))$



The next conditional posterior density I will calculate is 

$f(\eta|y_{ij},log(v_i),\mu,\nu,\lambda) \propto f(\eta)\prod_{i=1}^{n}f(log(v_{i})|\mu,\nu,\lambda,\eta)$

Here, $\eta = \frac{1}{\phi^2}$, which only shows up in the conditional density $f(logv_{i}|\mu,\nu,\lambda,\eta)$. I only take the product over all the individuals, i, since the seven traits, j, do not enter these densities. I will use the density given for $log(v_i) \sim N(\nu, \phi^2)$ instead of $v_i \sim logN(\nu, \phi^2)$ when solving for this conditional density, since it makes calculations more simple.


$f(\eta|log(y_{ij}),log(v_i),\mu,\nu,\lambda) \propto \eta^{c-1} e^{-d\eta} \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\phi^2}} e^{-\frac{(logv_i-\nu)^2}{2\phi^2}}$ 


$\propto \eta^{c-1} e^{-d\eta} \prod_{i=1}^{n} \eta^{\frac{1}{2}} e^{-\frac{\eta}{2}(log(v_i)-\nu)^2}$


$\propto \eta^{c-1} e^{-d\eta} \eta^{\frac{n}{2}} e^{-\frac{\eta}{2}\sum_{i=1}^{n}(log(v_i)-\nu)^2}$


$\propto \eta^{c+\frac{n}{2}-1} e^{-d\eta-\frac{\eta}{2}\sum_{i=1}^{n}(log(v_i)-\nu)^2}$


Therefore $f(\eta|y_{ij},log(v_i),\mu,\nu,\lambda) \sim Ga(c+\frac{n}{2}, d + \frac{1}{2}\sum_{i=1}^{n}(log(v_i)-\nu)^2)$



The next conditional posterior density I will calculate is 

$f(\mu|y_{ij},v_i,\nu,\lambda,\eta) \propto f(\mu) \prod_{i,j}f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)$

This is the form of the conditional density, because $\mu$ only shows up in $f(\mu)$ and $f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)$.


$f(\mu|y_{ij},v_i,\nu,\lambda,\eta) \propto  \frac{1}{\sqrt{2\pi\xi^2}} e^{-\frac{\mu^2}{2\xi^2}} \prod_{i,j}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}{2\sigma^2}}$


$\propto e^{-\frac{\mu^2}{2\xi^2}} e^{-\frac{\lambda}{2} \sum_{i,j}(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}$


I will call $log(y_{ij})-log(\frac{v_i}{v_i+x_{ij}}) = K_{ij}$ and $\frac{1}{\xi^2} = \tau$ which gives 


$\propto e^{-\frac{1}{2}(\tau \mu^2 - \lambda \sum_{i,j}(K_{ij} - \mu)^2)}$


$\propto e^{-\frac{1}{2}(\tau \mu^2 - \lambda \sum_{i,j}(K_{ij}^2 - 2\mu K_{ij} + \mu^2))}$


$\propto e^{-\frac{1}{2}(\tau \mu^2 - \lambda \sum_{i,j}K_{ij}^2 + 2\lambda \mu \sum_{i,j}K_{ij} - \lambda \mu^2 n m)}$


$\propto e^{-\frac{1}{2}(\mu^2 (\tau - \lambda nm) +\mu (2\lambda \sum_{i,j}K_{ij}) - \lambda \sum_{i,j}K_{ij}^2)}$


$\propto e^{-\frac{(\tau - \lambda nm)}{2}(\mu^2 +\mu \frac{(2\lambda \sum_{i,j}K_{ij})}{(\tau - \lambda nm)} - \frac{\lambda \sum_{i,j}K_{ij}^2}{(\tau - \lambda nm)})}$


$\propto e^{-\frac{(\tau - \lambda nm)}{2}(\mu^2 -\mu \frac{(2\lambda \sum_{i,j}K_{ij})}{(\lambda nm- \tau)} + \frac{\lambda \sum_{i,j}K_{ij}^2}{(\lambda nm- \tau)^2})}$


$\propto e^{-\frac{(\tau - \lambda nm)}{2}(\mu - \frac{\lambda \sum_{i,j}K_{ij}}{(\lambda nm- \tau)})^2}$


Therefore $f(\mu|y_{ij},v_i,\nu,\lambda,\eta) \sim N(\frac{\lambda \sum_{i,j}K_{ij}}{(\lambda nm- \tau)}, \frac{1}{(\tau - \lambda nm)})$

substituting back in $log(y_{ij})-log(\frac{v_i}{v_i+x_{ij}}) = K_{ij}$ and $\frac{1}{\xi^2} = \tau$,

$f(\mu|y_{ij},v_i,\nu,\lambda,\eta) \sim N(\frac{\lambda \sum_{i,j}(log(y_{ij})-log(\frac{v_i}{v_i+x_{ij}}))}{(\lambda nm- \frac{1}{\xi^2})}, \frac{1}{(\frac{1}{\xi^2} - \lambda nm)})$




The next conditional posterior densitu I will calculate is

$f(\nu|y_{ij},v_i,\mu,\lambda,\eta) \propto f(\nu) \prod_{i=1}^{n}f(log(v_{i})|\mu,\nu,\lambda,\eta)$

I will again use $log(v_i) \sim N(\nu, \phi^2)$ for simplicity of the calculation, keeping in mind $\nu$ is the parameter I'm interested in.


$f(\nu|y_{ij},v_i,\mu,\lambda,\eta) \propto \frac{1}{\sqrt{2\pi\psi^2}} e^{-\frac{\nu^2}{2\psi^2}} \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\phi^2}} e^{-\frac{(logv_i-\nu)^2}{2\phi^2}}$


$\propto e^{-\frac{\nu^2}{2\psi^2}} \prod_{i=1}^{n} e^{-\frac{\eta}{2}(log(v_i)-\nu)^2}$


$\propto e^{\frac{1}{2}(\frac{1}{\psi^2}\nu^2 - \eta \sum_i(log(v_i)-\nu)^2)}$

I will set $\frac{1}{\psi^2} = A$.


$\propto e^{-\frac{1}{2}(A\nu^2 - \eta \sum_i(log(v_i)^2 - 2log(v_i)\nu + \nu^2))}$


$\propto e^{-\frac{1}{2}(A\nu^2 - \eta \sum_i(log(v_i)^2) + 2\eta\nu \sum_ilog(v_i) - \eta n \nu^2))}$


$\propto e^{-\frac{1}{2}(\nu^2(A-\eta n) - \nu(2\eta \sum_ilog(v_i)) - \eta\sum_i(log(v_i)^2)))}$


$\propto e^{-\frac{(A-\eta n)}{2} (\nu^2 - \nu(\frac{2\eta \sum_ilog(v_i)}{\eta n-A}) + \frac{\eta \sum_ilog(v_i)^2}{(\eta n-A)^2})}$


$\propto e^{-\frac{(A-\eta n)}{2}(\nu^2 - \frac{\eta \sum_ilog(v_i)}{(\eta n-A)})^2}$


Therefore, $f(\nu|y_{ij},v_i,\mu,\lambda,\eta) \sim N(\frac{\eta \sum_ilog(v_i)}{\eta n-A}, \frac{1}{A-\eta n})$



The last conditional posterior density I will calculate is for

$f(v_{i}|log(y_{ij}, \mu,\nu,\lambda,\eta) \propto f(v_{i}|\mu,\nu,\lambda,\eta) \prod_{i,j}f(log(y_{ij})|v_i,\mu,\nu,\lambda,\eta)$


I will be using the distribution for $v_i$, not for $log(v_i)$, because contained within $log(y_{ij})$ is both $log(v_i)$ and $log(v_i + x_{ij})$, which would casue some issues if I only find the conditional on $log(vi)$. Recall that $v_i \sim logN(\nu, \phi^2)$. I only take the product across j = 1,...,m of $log(y_{ij})$ since $v_i$ is only indexed across i = 1,...,n, where m=7 and n=86. 

This renders a conditional density of 

$f(v_{i}|log(y_{ij}, \mu,\nu,\lambda,\eta) \propto  \frac{1}{v_i \sqrt{2\pi \phi^2}} e^{-\frac{(logv_i -\nu)^2}{2\phi^2}} \prod_{j}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(log(y_{ij})-\mu-log(\frac{v_i}{v_i+x_{ij}}))^2}{2\sigma^2}}$


$\propto \frac{1}{v_i} e^{-\frac{\eta}{2} (logv_i - \nu)^2} e^{-\frac{\lambda}{2} \sum_j (y_{ij}-\mu -log(v_i)+log(v_i+x_{ij})^2}$

$\propto \frac{1}{v_i} e^{-\frac{1}{2}(\eta (logv_i - \nu)^2 - \lambda \sum_j (y_{ij}-\mu -log(v_i)+log(v_i+x_{ij})^2)}$


The form of this conditional density makes me believe that is should follow some log Normal distribution, but I am unable to find the exact parameters of the conditional distribution, since I am unable to isolate $v_i$ in the above equation.
This means that I will have to implement a Metropolis step in order to sample from this conditional density, since I cannot sample directly from it. I will need to implement the Metropolis Hastings algorithm seperately for the 86 $v_i$ estimates, since $v_i$ is i.i.d..




#Gibbs Sampler with MH for all other conditionals

Now that I have found the conditional densities, I must take samples from them, so I can estimate $f(log(y_{ij}))$. I will implement a Gibbs Sampler to estimate the conditional densities. The Gibbs Sampler can be implemented since there are more than 2 parameters in this model, and I am able to sample directly from all the conditional densities besides $f(v_{i}|log(y_{ij}, \mu,\nu,\lambda,\eta)$, where I will introduce a Metropolis step.


Some information on why we can use a Gibbs Sampler follows. In a Gibbs framework the we define the transition density $(\lambda, \eta, \nu, \mu, v_1,...,v_{86}) \rightarrow (\lambda', \eta', \nu', \mu', v_1',...,v_{86}')$ by $P(\lambda', \eta', \nu', \mu', v_1',...,v_{86}' | \lambda, \eta, \nu, \mu, v_1,...,v_{86})$


$P(\lambda', \eta', \nu', \mu', v_1',...,v_{86}' | \lambda, \eta, \nu, \mu, v_1,...,v_{86}) = f(\lambda' | \eta, \nu, \mu, v_1,...,v_{86}) *f(\eta' | \lambda', \nu, \mu, v_1,...,v_{86})*f(\nu' | \lambda', \eta', \mu, v_1,...,v_{86}) * f(\mu' | \lambda', \eta', \nu',v_1,...,v_{86}) *f(v_1' | \lambda', \eta', \nu', \mu', v_2,...,v_{86}) *...f(v_{86}' | \lambda', \eta', \nu', \mu', v_1',...,v_{85}')$
    

I will then take N many iterations of the transition densities. We want a transition density that will give us a stationary density 
$f(\lambda', \eta', \nu', \mu', v_1',...,v_{86}') = \int P(\lambda', \eta', \nu', \mu', v_1',...,v_{86}' | \lambda, \eta, \nu, \mu, v_1,...,v_{86}) f(\lambda, \eta, \nu, \mu, v_1,...,v_{86})$. 
I will simplify this to 
$f(\theta', v_i') = \int \int P(\theta', v_i'| \theta, v_i)f(\theta,v_i) = \int \int f(v_i'|\theta', v_i)f(\theta'|\theta, v_i)f(\theta,v_i)d\theta dv_i$

Using $f(\theta' |\theta, v_i) = \frac{f(\theta',\theta,v_i)}{f(\theta,v_i)}$, I can now simplify to 

$f(\theta', v_i') = \int \int f(v_i'|\theta', v_i)f(\theta',\theta,v_i)d\theta dv_i$

$f(\theta', v_i') = \int f(v_i'|\theta', v_i)f(\theta',v_i)dv_i = \int \frac{f(v_i',\theta', v_i)}{f(\theta',v_i)}f(\theta',v_i)dv_i$

$\int f(v_i',\theta', v_i) dv_i = f(\theta', v_i')$

This confirms that the transition density does render a stationary density, $f(\theta', v_i')$. 

We may get stuck when trying to do these calculations in the Gibbs framework since $v_i$ cannot be sampled in that framework. This means to sample from $f(v_i|log(y_{ij}, \mu,\nu,\lambda,\eta)$, we will need a transition density that satisfies $f(v_i)q(v_i'|v_i) = f(v_i')q(v_i|v_i')$. This means $p(v_i'|v_i)$ shows up in the form $p(v_i'|v_i) = \alpha(v_i,v_i')q(v_i')+(1-r(v_i))1(v_i'=v_i)$.

Plugging in for $p(v_i'|v_i)$, we get $f(v_i)\alpha(v_i,v_i')q(v_i') = f(v_i')\alpha(v_i,v_i')q(v_i)$. In the Metropolis Hastings algorithm, $\alpha = min(1, \frac{f(v_i')q(v_i|v_i')}{f(v_i)q(v_i'|v_i)})$. 

The algorithm I will take to implement the Metropolis Hastings step is
  1) Decide current state $v_{i,n}$ (this will be the entries in my generated $v_i$ matrix)
  2) Take a proposal state such that $q(v_i'|v_i) \sim logNormal(v_i' | v_i, \beta^2)$
  3) Take $u \sim U(0,1)$
  4) If $u < \alpha(v_i,v_i')$, 
      then $v_{i,n+1} = v_i'$, 
      else $v_{i,n+1} = v_{i,n}$ 


I will implement this MH step for $v_i$ for every $i=1,...,86$. To obtain the estimate for each $v_i$, I simply will take the mean of all the respective $v_i$ generated from my MH algorithm. I iterated both the Gibbs Sampler and Metropolis Hastings step 110,000 times with a 10,000 sample burnin, to render 100,000 generated estimates for each parameter.


```{r, include=FALSE}
n <- 86
m <- 7
v <- as.matrix(colMeans(V.est))
x.matrix = matrix(rep(1:7,86), byrow=TRUE, ncol=7)
T = function(v, NU, ETA){(sqrt(ETA)/(v*sqrt(2*pi))) * exp(-ETA*((log(v) - NU)^2)/2)}


# sample from the joint posterior (mu, tau | data)
LAM <- NU <- MU <- ETA <- rep(0, 110000)
burnin <- 10000    # burnin
LAM[1] <- NU[1] <- MU[1] <- ETA[1] <- 0  # initialisation
set.seed(20)

for(i in 2:110000) {   
    ETA[i] <- rgamma(1, 3+(n/2), rate= (2 + 0.5 * colSums(as.matrix((v[,1] - NU[i-1])^2)))^2) #squared rate 
    
    LAM[i] <- rgamma(1, 2+(n*m/2), rate = (1 + 0.5* colSums(as.matrix(colSums(as.matrix((logY[,1:7] - MU[i-1] -log(v[,1])+log(v[,1] + x.matrix[,1:7])^2))))))^2) #I squared the rate parameter here
    
    MU[i]  <- rnorm(1, (LAM[i]*colSums(as.matrix(colSums(as.matrix(logY[,1:7]-log(v[,1])+log(v[,1] + x.matrix[,1:7]))))))/(LAM[i]*m*n -1),  (1/(1-LAM[i]*n*m))^2)
    
    NU[i]  <- rnorm(1, (ETA[i]*colSums(v))/(ETA[i]*n - 2), (1/(2-ETA[i]*n))^2)
    
    
    MH = function(niter, startval, proposalsd){
      VI = rep(0,niter)
      VI[1] = startval     
    for(i in 2:niter){
        currentx = VI[i-1]
        proposedx = rlnorm(1,mean=currentx,sd=proposalsd) 
        A = min(1,(T(proposedx, NU[i], ETA[i])*dlnorm(currentx, mean=proposedx, sd=proposalsd))/(T(currentx, NU[i], ETA[i])*dlnorm(proposedx, mean=currentx, sd=proposalsd)))
      if(runif(1)<A){
        VI[i] = proposedx       # accept move with probabily min(1,A)
      } else {
       VI[i] = currentx        # otherwise "reject" move, and stay where we are
      }
    }
    return(VI)
    }
}
LAM1  <- LAM[-(1:burnin)]   # remove burnin
ETA1  <- ETA[-(1:burnin)] 
NU1   <- NU[-(1:burnin)]
MU1   <- MU[-(1:burnin)]
```


#5) Report output of chain


In order to determine which $\beta^2$ I want to include in the proposal density of the MH step, $q(v_i'|v_i) \sim logNormal(v_i' | v_i, \beta^2)$, I tested several different values and picked the one that had the lowest, or quickest converging ACF. A plot of the ACF's for $\beta^2$ = (0.1, 1.5, 2, 3, 5, 10, 50). I am using the Metropolis Hastings estimate for $v_1$ in the plots below. Since each $v_i$ is i.i.d., I assumed that a $\beta^2$ for $v_1$ will also be reasonable for the other 85 $v_i$'s.



```{r, echo=FALSE}
A1 <- MH(100000, v[1,], 0.1)
A2 <- MH(100000, v[1,], 1.5)
A3 <- MH(100000, v[1,], 2)
A4 <- MH(100000, v[1,], 3)
A5 <- MH(100000, v[1,], 5)
A6 <- MH(100000, v[1,], 10)
A7 <- MH(100000, v[1,], 40)

par(mfrow=c(2,2))
acf(A1, main = "ACF with 0.1")
acf(A2, main = "ACF with 1.5")
acf(A3, main = "ACF with 2")
acf(A4, main = "ACF with 3")
acf(A5, main = "ACF with 5")
acf(A6, main = "ACF with 10")
acf(A7, main = "ACF with 50")
```



The ACF plots above reveal that a $\beta^2 = 3$ has the least autocorrelated samples of $v_i$, since the ACF function converges to zero the fastest compared to other $\beta^2$'s. The graph below has lines of three MH estimates of $v_1$ for $\beta^2$'s equal to 0.1, 3, and 50. We see that MH estimates from proposals with very low and very large variances are persistent and highly correlated to past samples. The MH estimate with an appropriately chosen proposal density variance converges to the true value of $v_1$ much faster. The MH estimate of $v_1$ is resembled by the red horizontal line on the graph.


```{r, echo=FALSE}
plot(A5,type="l", ylab = "Estimates of Target Distribution", 
     xlab = "Iterations", main="MH Estimates with Differing Proposal Density Variance")
lines(A1,col='green')
lines(A7,col='blue')
abline(h=2.7727582, col="red")
```



Below, I graphed the convergence of the estimates for the four other parameters, $\lambda, \eta, \nu, \mu$, estimated using the Gibbs Sampler. The horizontal red line is the estimate value of the respective parameters.

```{r, echo=FALSE}
par(mfrow= c(2,2))
plot(LAM1, type="l", main= "GS Estimate of lambda", ylim = c(0,300))
abline(h=1.199456e+02, col="red")
plot(ETA1, type="l", main= "GS Estimate of eta")
abline(h=1.149280e+01, col="red")
plot(NU1, type="l", main= "GS Estimate of nu")
abline(h=1.002464, col="red")
plot(MU1, type="l", main= "GS Estimate of mu", ylim = c(-400,400))
abline(h=-3.993270e+06 , col="red")
```



To finalize the output from the chain, below is a table of the Monte Carlo estimates for each parameter in the model. The estimates were calculated by the average of the 100,000 generated estimates from either the Gibbs Sampler or Metropolis Hastings step for each respective parameter.


```{r, include=FALSE}
set.seed(3)
v.matrix= matrix(nrow=100000, ncol=86)

for(k in 1:86){
    v.matrix[1:100000,k] = MH(100000, v[k,], 3)
}

EST <- as.matrix(cbind(LAM1, ETA1, NU1, MU1, v.matrix))
```

```{r, echo=FALSE}
colMeans(EST)
```





#6) Estimating v_87


Two pieces of information given was that $y_{87,1}=0.257$ for $x_{87,1}=1$. This will help to determine the value of $v_[87]$, along with the Monte Carlo estimates that were found above.

Since I have found the estimates for $(\lambda, \mu, \eta, \nu)$, I do not need to re-run the Gibbs Sampler again on all the parameters to find $v_87$. 
I will use the Metropolis Hastings step to generate 100,000 samples of $v_{87}$, then average over those samples to find the estimate for $v_{87}$. In order to implement the Metropolis Hastings step, I appended an extra row to the $loy(y_{ij})$ matrix and to the $x_{ij} = j$ vector, to make a place to store the estimate. 

I will run the MH algorithm for the $v_i$'s, using the estimated for the parameters $(\lambda, \mu, \eta, \nu)$, and find an estimate of $v_{87}$ given the other fixed parameters, $y_{87,1}=0.257$ for $x_{87,1}=1$, too. I specified a $\beta^2$ of the proposal density as 3 as well.

```{r, echo=FALSE}
MH = function(niter, startval, proposalsd){
      VI = rep(0,niter)
      VI[1] = startval     
    for(i in 2:niter){
        currentx = VI[i-1]
        proposedx = rlnorm(1,mean=currentx,sd=proposalsd) 
        A = min(1,(T(proposedx, 1.00246, 1.149280e+01)*dlnorm(currentx, mean=proposedx, sd=proposalsd))/(T(currentx, 1.00246, 1.149280e+01)*dlnorm(proposedx, mean=currentx, sd=proposalsd)))
      if(runif(1)<A){
        VI[i] = proposedx       # accept move with probabily min(1,A)
      } else {
       VI[i] = currentx        # otherwise "reject" move, and stay where we are
      }
    }
    return(VI)
}

mhmean <- mean(MH(100000, 0.257, 3))
plot(MH(10000, 0.257, 3), type='l', main = "Estimate of V_87", ylab="Estimate")
abline(h=mhmean, col="red")
```

This process rendered me an estimate for $v_{87} = 2.798$ for an idividual with the parameters specified above. As we can see, under this proposal density variance, the estimate for $v_87$ does not appear to be highly correlated with past estimates. 

































