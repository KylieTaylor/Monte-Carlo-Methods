---
title: "Monte Carlo Assignment 6 and Final"
author: "Kylie Taylor"
date: "5/9/2019"
output: pdf_document
---

##Introduction

We have two models

Model 1 (M1):

$$ y_i = log(1+exp(\beta_0 + \beta_1 x_{1i})) + \sigma\epsilon_i $$

Model 2 (M2):

$$ y_i = log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})) + \sigma\epsilon_i $$


The data shows up as $(y_i, x_{1i}, x_{2i})$, each having n=50 observations. The error term, $\epsilon_i$ is distributed i.i.d. standard normal, N(0,1). 


We are given the priors for each model as $P(M1) = P(M2) = \frac{1}{2}$. Each $\beta$ term $(\beta_0, \beta_1, \beta_2)$ have priors 

$$ f(\beta_0)=f(\beta_1)=f(\beta_2)= N(0,5^2) $$

The goal of this assignment is to find $P(M1|y_i, x_{1i}, x_{2i})$, thus $P(M2|y_i, x_{1i}, x_{2i}) = 1- P(M1|y_i, x_{1i}, x_{2i})$ using two Monte Carlo methods

1) Bayes Factor to compare models
2) Metropolis Hastings algorithm to jump between models


After accomplishing that, our goal is to find the predictive density for somebody with $x_{1,51} = 0.23$ and $x_{2,51} = -1.2$. 




##Data

The first step I will take is to inspect the data. "V1" corresponds to $y_i$ values, "V2" corresponds to $x_{1i}$ values, and "V3" corresponds to $x_{2i}$ values. $y_i$ has mean 0.986, minimum value -1.8619, and max value 4.0021. $x_{1i}$ has mean 0.52240, minimum value 0.03637, and max value 0.98170. $x_{2i}$ has mean 0.435406, minimum value 
0.004233, and max value 0.998834. 


I plotted the density of the three variables in one plot below. We see that $x_{1i}$ and $x_{2i}$ have much smaller variance than $y_i$. 

```{r, include=FALSE}
data <- read.csv("https://raw.githubusercontent.com/KylieTaylor/Monte-Carlo-Methods/master/final.dat.txt", sep = "")
DATA <- as.matrix(data)
```

```{r, echo=FALSE}
set.seed(9)
library(ggplot2)
ggplot(data=data) +geom_density(aes(x=V1, col="Y")) + geom_density(aes(x=V2, col="X1")) +geom_density(aes(x=V3, col="X2")) +ggtitle("Histograms of Y, X1 and X2") + xlab("Variables") +ylab(" ") +xlim(c(-3,5))
```






##Conditional Posterior Densities

My next step will be to find the conditional posterior densities of the parameters. The parameters in these models are $(\beta_0, \beta_1, \lambda)$ for model 1, and $(\beta_0, \beta_1, \beta_2, \lambda)$ for model 2. 


For model 1, the joint density is 

$$f(\beta_0,\beta_1,\lambda,y_i|x_{1i})=f(\lambda)f(\beta_0)f(\beta_1)\prod_ik(y_i|\beta_0,\beta_1,\lambda)$$


The prior for $\lambda$ will be given as 
$$f(\lambda)=Ga(\frac{1}{2},\frac{1}{2})$$.



The likelihood is 
$$ \prod_i N(y_i|log(1+exp(\beta_0 + \beta_1 x_{1i})), \sqrt{\frac{1}{\lambda}} )$$



The posterior for $\beta_0$ is
$$ f(\beta_0|\lambda,\beta_1,y_i)=f(\beta_0)\prod_ik(y_i|\beta_0,\beta_1,\lambda)$$
$$ \propto e^{-\frac{\beta_0^2}{50}} e^{-\frac{\lambda n}{2}\sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2}  $$

$$ \propto e^{-\frac{1}{2} (\frac{\beta_0^2}{25} + \lambda n \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2)}  $$



This follows for $\beta_1$ 

$$ f(\beta_1|\beta_1,\lambda,y_i)\propto e^{-\frac{1}{2} (\frac{\beta_1^2}{25} + \lambda n \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2)} $$



The posterior for $\lambda$ is

$$f(\lambda|\beta_0,\beta_1,y_i) \propto \lambda^{\frac{1}{2} -1} e^{-\frac{1}{2}\lambda} \lambda^{\frac{n}{2}} e^{-\frac{\lambda}{2}\sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2}$$
$$ \propto  \lambda^{\frac{1}{2} -\frac{n}{2}} e^{-\lambda (\frac{1}{2} + \frac{n}{2} \sum_i (y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2)}$$
$$ f(\lambda|\beta_0,\beta_1,y_i)= Ga(\frac{1}{2} + \frac{n}{2}, \frac{1}{2} + \frac{1}{2} \sum_i (y_i - log(1+exp(\beta_0 + \beta_1 x_{1i})))^2) $$



For model 2, the joint density is 

$$f(\beta_0,\beta_1,\beta_2,\lambda,y_i|x_{1i})=f(\lambda)f(\beta_0)f(\beta_1)f(\beta_2)\prod_ik(y_i|\beta_0,\beta_1,\beta_2,\lambda)$$


The posterior for $\beta_0$ is

$$f(\beta_0|\beta_1,\beta_2,\lambda,y_i) \propto e^{-\frac{1}{2} (\frac{\beta_0^2}{25} + \lambda \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})))^2)}$$



For $\beta_1$

$$ f(\beta_1|\beta_0,\beta_2,\lambda,y_i) \propto e^{-\frac{1}{2} (\frac{\beta_1^2}{25} + \lambda \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})))^2)}$$



For $\beta_2$

$$ f(\beta_2|\beta_0,\beta_1,\lambda,y_i) \propto e^{-\frac{1}{2} (\frac{\beta_2^2}{25} + \lambda  \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})))^2)}$$


For $\lambda$

$$ f(\lambda|\beta_0,\beta_1,\beta_2,y_i)= Ga(\frac{1}{2} + \frac{n}{2}, \frac{1}{2} + \frac{1}{2} \sum_i (y_i - log(1+exp(\beta_0 + \beta_1 x_{1i}+ \beta_2 x_{2i})))^2) $$





##Posterior Estimations


In order to estimate every $\beta$ term in both model 1 and 2, I must introduce a Metropolis Hastings step. To sample some $\beta_k$ for k = (0,1,2), I will implement the following MH step


    given current state b_n
    take proposal, b' ~ q
    take u ~ U(0,1)
    if u < a(b,b')
      b_{n+1} = b'
    else
      b_{n+1} = b_n
      


I will use a normal proposal density, and the 
$$ \alpha(\beta_k,\beta_k') = min(1, \frac{f(\beta_k'|\beta_{-k},\lambda,y_i)*q(\beta_k|\beta_k',e)} {f(\beta_k|\beta_{-k},\lambda,y_i)*q(\beta_k'|\beta_k,e)}) $$

$$ \propto min(1, \frac{e^{-\frac{1}{2} (\frac{\beta_k'^2}{25} + \lambda  \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})))^2)}} {e^{-\frac{1}{2} (\frac{\beta_k^2}{25} + \lambda  \sum_i(y_i - log(1+exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i})))^2)}}) $$

In order to sample the $\lambda$ I can use a Gibbs Framework. I iterate through 100,000 samples of the parameters with a 20,000 burnin, resulting in 80,000 final samples of all the parameters. 


Trace plots and histograms of all the parameters are included below. The trace plots below reveal that the Metroplolis Hastings step for sampling the $\beta$'s and Gibbs step for sampling the $\lambda$ appear to move well and have an appropriate proposal variance. The histograms reveal there are no out-of-ordinary distributions for the parameters, and are centered around some high density point, likely the mean.  


```{r, include=FALSE}
n=50
set.seed(87201)
e <- rnorm(50)
Y1 = function(b0, b1){log(1+exp(b0 + b1*(DATA[,2])))}
Y2 = function(b0, b1, b2){log(1+exp(b0 + b1*(DATA[,2]) + b2*(DATA[,3])))}
F1 <- function(b,l,b0, b1){-0.5*exp(((b^2)*0.04) + l*sum((DATA[,1] - log(1 +exp(b0 + b1*DATA[,2])))^2))}
F2 <- function(b,l,b0, b1, b2){-0.5*exp(((b^2)*0.04) + l*sum((DATA[,1] - log(1 +exp(b0 + b1*DATA[,2] + b2*DATA[,2])))^2))}
F12 <- function(b){exp(-(b^2)/50)}


t = 100000
burnin = 0.2*t

#initialize vectors
b01 <- b11 <- b21 <- LAM1 <- b02 <- b12 <- b22 <- LAM2 <- rep(0,t)
b01[1] <- b11[1] <- b21[1] <- b02[1] <- b12[1] <- b22[1] <- LAM1[1] <- LAM2[1] <- 1
```


```{r, include=FALSE}
for(i in 2:t){
  #for Lambda in model 1
    LAM1[i] <- rgamma(1, shape = 0.5+ 25,  rate = 0.5 + 0.5*sum((DATA[,1] - log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])))^2))
    
    #for b0 in model 1
    currentB01 = b01[i-1]
    proposedB01 = rnorm(1, mean=currentB01, sd=2) 
    
    NUMb01 <- ((exp(-0.5*((proposedB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(proposedB01 + b11[i-1]*DATA[,2])))^2))) *dnorm(currentB01, mean=proposedB01, sd=2)) 
    
    DENOMb01 <- ((exp(-0.5*((currentB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1]-log(1 +exp(currentB01 + b11[i-1]*DATA[,2])))^2)))*dnorm(proposedB01, mean=currentB01, sd=2))
    
    NDb01 <- NUMb01/DENOMb01
    
    A = min(1, NDb01)
    if(runif(1)<A){
      b01[i] = proposedB01       # accept move with probabily min(1,A)
    }else{
      b01[i] = currentB01      # otherwise "reject" move, and stay where we are
    }
    
    #for b1 in model 1
    currentB11 = b11[i-1]
    proposedB11 = rnorm(1,mean=currentB11, sd=2) 
    
    NUMb11 <- ((exp(-0.5*((proposedB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + proposedB11*DATA[,2])))^2))) *dnorm(currentB11, mean=proposedB11, sd=2))
    
    DENOMb11 <- ((exp(-0.5*((currentB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + currentB11*DATA[,2])))^2))) *dnorm(proposedB11, mean=currentB11, sd=2))
    
    NDb11 <- NUMb11/DENOMb11
    A = min(1, NDb11)
    if(runif(1)<A){
      b11[i] = proposedB11       # accept move with probabily min(1,A)
    }else{
      b11[i] = currentB11     # otherwise "reject" move, and stay where we are
    }
}

beta01 <- b01[-(1:burnin)]
beta11 <- b11[-(1:burnin)]
lam1 <- LAM1[-(1:burnin)]

beta0M1 <- mean(beta01)
beta1M1 <- mean(beta11)
lamM1 <- mean(lam1)

beta0M1
beta1M1 
lamM1
```


```{r, include=FALSE}
for(i in 2:t){
  
    LAM2[i] <- rgamma(1, 0.5 + 25, 0.5+0.5*sum((DATA[,1]-log(1+exp(b02[i-1] + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))
  
  #for b0 in model 2
    currentB02 = b02[i-1]
    proposedB02 = rnorm(1, mean=currentB02, sd=2) 
    
    NUMb02 <- ((exp(-0.5*((proposedB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(proposedB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB02, mean=proposedB02, sd=0.7)) 
    
    DENOMb02 <- ((exp(-0.5*((currentB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(currentB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) * dnorm(proposedB02, mean=currentB02, sd=0.7))
    
    NDb02 <- NUMb02/DENOMb02
    
    A = min(1,NDb02)
    if(runif(1)<A){
      b02[i] = proposedB02       # accept move with probabily min(1,A)
    }else{
      b02[i] = currentB02      # otherwise "reject" move, and stay where we are
    }
    
    #for b1 in model 2
    currentB12 = b12[i-1]
    proposedB12 = rnorm(1,mean=currentB12, sd=2) 
    
    NUMb12 <- ((exp(-0.5*((proposedB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + proposedB12*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB12, mean=proposedB12, sd=0.7)) 
    
    DENOMb12 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + currentB12*DATA[,2] + b22[i-1]*DATA[,3])))^2)))*dnorm(proposedB12, mean=currentB12, sd=0.7))
    
    NDb12 <- NUMb12/DENOMb12
    
    A = min(1, NDb12)
    if(runif(1)<A){
      b12[i] = proposedB12       # accept move with probabily min(1,A)
    }else{
      b12[i] = currentB12     # otherwise "reject" move, and stay where we are
    }
    
     #for b2 in model 2
    currentB22 = b22[i-1]
    proposedB22 = rnorm(1,mean=currentB22, sd=2)
    
    NUMb22 <- ((exp(-0.5*((proposedB22^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + proposedB22*DATA[,3])))^2))) *dnorm(currentB22, mean=proposedB22, sd=0.7)) 
    
    DENOMb22 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + currentB22*DATA[,3])))^2)))*dnorm(proposedB22, mean=currentB22, sd=0.7))
    
    NDb22 <- NUMb22/DENOMb22
    
    A = min(1, NDb22)
    if(runif(1)<A){
      b22[i] = proposedB22       # accept move with probabily min(1,A)
    }else{
      b22[i] = currentB22      # otherwise "reject" move, and stay where we are
    }
}

beta02 <- b02[-(1:burnin)]
beta12 <- b12[-(1:burnin)]
beta22 <- b22[-(1:burnin)]
lam2 <- LAM2[-(1:burnin)]

beta0M2 <- mean(beta02)
beta1M2 <- mean(beta12)
beta2M2 <- mean(beta22)
lamM2 <- mean(lam2)

beta0M2
beta1M2
beta2M2
lamM2
```


```{r, echo=FALSE}
par(mfrow= c(2,2))
plot(lam1, type='l')
hist(lam1,100)
plot(beta01, type = 'l')
hist(beta01,100)
plot(beta11, type = 'l')
hist(beta11,100)
plot(lam2, type='l')
hist(lam2,100)
plot(beta02, type = 'l')
hist(beta02,100)
plot(beta12, type = 'l')
hist(beta12,100)
plot(beta22, type = 'l')
hist(beta22,100)
```






##Bayes Factor


Remember that the posterior distribution for some $\theta$ is $f(\theta|data) \propto f(\theta)k(data|\theta)$. The goal in Bayes Factor is to estimate $f(\tilde\theta|data)$ given the posterior samples $(\theta^{(1)},...,\theta^{(m)})$, which I obtained above using a Gibbs sampler for $\lambda$ and Metropolis Hastings framework for $(\beta_0, \beta_1, \beta_2)$. 

Using the estimates from the posteriors for Model 1 and Model 2, I will calculate the Bayes Factor to find P(M1|data). Recall that the equation for the posterior odds is

$$ \frac{P(M1|y_i,x_{1i},x_{2i})}{P(M2|y_i,x_{1i},x_{2i})} = \frac{K(y_i,x_{1i},x_{2i}|M1)}{K(y_i,x_{1i},x_{2i}|M2)} * \frac{P(M1)}{P(M2)}$$


Again, the data shows up as $(y_i,x_{1i},x_{2i})$. The posterior odds is 
$$\frac{P(M1|y_i,x_{1i},x_{2i})}{P(M2|y_i,x_{1i},x_{2i})}$$


The Bayes Factor is 

$$ \frac{K(y_i,x_{1i},x_{2i}|M1)}{K(y_i,x_{1i},x_{2i}|M2)}$$


And the Prior Odds are

$$ \frac{P(M1)}{P(M2)}= \frac{\frac{1}{2}}{\frac{1}{2}} =1  $$

This term will drop away from the equation, also P(M2|data) = 1-P(M1|data), giving us the Posterior Odds equal to the Bayes Factor. 

$$\frac{P(M1|y_i,x_{1i},x_{2i})}{1-P(M1|y_i,x_{1i},x_{2i})} = \frac{K(y_i,x_{1i},x_{2i}|M1)}{K(y_i,x_{1i},x_{2i}|M2)}$$



The Bayes Factor is calculated using the following equations. The integral $I=K(y_i,x_{1i},x_{2i}|M_i)$ is known as the marginal likelihood for a given model.


For model 1:

$$I=K(y_i,x_{1i},x_{2i}|M1)=\int f(\lambda)f(\beta_0)f(\beta_1)\prod_i k(y_i|\beta_0,\beta_1,\lambda)d\lambda d\beta_0 d\beta_1$$


Since the marginal likelihood is the normalizing constant of the posterior density, we can write the following equation, which is referred to as the basic marginal likelihood identity. 


$$ K(y_i,x_{1i},x_{2i}|M1)=\frac{f(\tilde\lambda)f(\tilde\beta_0)f(\tilde\beta_1)k(y_i|\tilde\beta_0,\tilde\beta_1,\tilde\lambda)}{f(\tilde\beta_0,\tilde\beta_1,\tilde\lambda,y_i|x_{1i},x_{2i})}  $$


Taking the log of the marginal likelihood identity and evaluating it at some $\tilde\theta$, returns

$$log(K(y_i,x_{1i},x_{2i}|M1)) = log(f(\tilde\lambda)) +log(f(\tilde\beta_0)) +log(f(\tilde\beta_1)) + log(k(y_i|\tilde\beta_0,\tilde\beta_1,\tilde\lambda)) - log(f(\tilde\beta_0,\tilde\beta_1,\tilde\lambda,y_i|x_{1i},x_{2i}))  $$


The value $\tilde\theta$ is taken as some high density point in the support of the posterior, which I use as the mean of the posterior estimates




The same steps follow natural for model 2:

$$I=K(y_i,x_{1i},x_{2i}|M2)=\int f(\lambda)f(\beta_0)f(\beta_1)f(\beta_2)\prod_i k(y_i|\beta_0,\beta_1,\beta_2,\lambda)d\lambda d\beta_0 d\beta_1d\beta_2$$


$$ K(y_i,x_{1i},x_{2i}|M2)=\frac{f(\tilde\lambda)f(\tilde\beta_0)f(\tilde\beta_1)f(\tilde\beta_2)k(y_i|\tilde\beta_0,\tilde\beta_1,\tilde\beta_2,\tilde\lambda)}{f(\tilde\beta_0,\tilde\beta_1,\tilde\beta_2,\tilde\lambda,y_i|x_{1i},x_{2i})}  $$


$$log(K(y_i,x_{1i},x_{2i}|M2)) = log(f(\tilde\lambda)) +log(f(\tilde\beta_0)) +log(f(\tilde\beta_1))+log(f(\tilde\beta_2)) + log(k(y_i|\tilde\beta_0,\tilde\beta_1, \tilde\beta_2 ,\tilde\lambda)) - log(f(\tilde\beta_0,\tilde\beta_1 ,\tilde\beta_2,\tilde\lambda,y_i|x_{1i},x_{2i}))  $$




The major issue with finding a value of $I$ is that $f(\tilde\theta|data)$ is hard to compute. Therefore I will implement the following equation to get an estimate for $f(\tilde\theta|data)$

$$ f(\tilde\theta|data) = \frac{\int\alpha(\theta,\tilde\theta)*q(\tilde\theta|\theta)*f(\theta|data)d\theta} {\int\alpha(\tilde\theta, \theta)*q(\theta|\tilde\theta) d\theta} $$

Where $\tilde\theta = (\tilde\beta_0, \tilde\beta_1, \tilde\beta_2, \tilde\lambda, \tilde y_i)$,  $\theta' = (\beta_0',\beta_1',\beta_2',\lambda', y_i')$ and data =$(y_i,x_{1i},x_{2i})$. 

This is a clever way of approximating $f(\tilde\theta|data)$, because the numerator satisfies the reversibility condition. The reversibility condition entails that $p(\theta^{(m)},\tilde\theta)f(\theta^{(m)}|data) = f(\tilde\theta|data)p(\tilde\theta, \theta^{(m)})$, where $p(\theta^{(m)},\tilde\theta) = \alpha(\theta,\tilde\theta)*q(\tilde\theta|\theta)$. This allows us to use the following estimations for the above integrals.

$$f(\tilde\theta|data) = \frac{\frac{1}{M} \sum_m  \alpha(\theta^{(m)}, \tilde\theta)*q(\tilde\theta|\theta^{(m)})} {\frac{1}{M} \sum_m  \alpha(\tilde\theta, \theta^{(m)})} $$


Here, the $\alpha(\theta^{(m)}, \tilde\theta)$ is equal to

$$ \alpha(\theta^{(m)},\tilde\theta)=min(1,\frac{f(\tilde\theta|data)*q(\tilde\theta|\theta^{(m)},\epsilon)}{f(\theta^{(m)}|data)*q(\theta^{(m)}|\tilde\theta, \epsilon)} )  $$


Plugging in for $f(\theta^{(m)}|data)$ and $f(\tilde\theta|data)$ we have the following fraction:

$$\frac{f(\tilde\theta|data)}{f(\theta^{(m)}|data)} = \frac{\frac{k(data|\tilde\theta)*f(\tilde\theta)}{f(data)}} {\frac{k(data|\theta^{(m)})*f(\theta^{(m)})}{f(data)}}  = \frac{k(data|\tilde\theta)*f(\tilde\theta)} {k(data|\theta^{(m)})*f(\theta^{(m)})}   $$



In terms of model 1 parameters,

$$ \frac{f(\tilde\beta_0, \tilde\beta_1, \tilde\lambda,  y_i|x_{1i},x_{2i})} {f(\beta_0^{(m)},\beta_1^{(m)},\lambda^{(m)},  y_i|x_{1i},x_{2i})} = \frac{\prod_i N(y_i|log(1+exp(\tilde\beta_0 + \tilde\beta_1 x_{1i})), \sqrt{\frac{1}{\tilde\lambda}}) f(\tilde\beta_0)f(\tilde\beta_1)f(\tilde\lambda)} {\prod_i N(y_i|log(1+exp(\beta_0^{(m)} + \beta_1^{(m)} x_{1i})), \sqrt{\frac{1}{\lambda^{(m)}}}) f(\beta_0^{(m)}) f(\beta_1^{(m)}) f(\lambda^{(m)})}$$



In terms of model 2 parameters,

$$ \frac{f(\tilde\beta_0, \tilde\beta_1, \tilde\beta_2,\tilde\lambda,  y_i|x_{1i},x_{2i})} {f(\beta_0^{(m)},\beta_1^{(m)},\beta_2^{(m)},\lambda^{(m)},  y_i|x_{1i})} = \frac{\prod_i N(y_i|log(1+exp(\tilde\beta_0 + \tilde\beta_1 x_{1i}+\tilde\beta_2 x_{2i})), \sqrt{\frac{1}{\tilde\lambda}}) f(\tilde\beta_0) f(\tilde\beta_1) f(\tilde\beta_2) f(\tilde\lambda)} {\prod_i N(y_i|log(1+exp(\beta_0^{(m)} + \beta_1^{(m)} x_{1i}+\beta_2^{(m)} x_{2i})), \sqrt{\frac{1}{\lambda^{(m)}}}) f(\beta_0^{(m)}) f(\beta_1^{(m)}) f(\beta_2^{(m)}) f(\lambda^{(m)})}$$



I will take normal proposal densities of the Metropolis step as 
$$q(\theta^{(m)}|\tilde\theta, \epsilon) \sim N(\tilde\theta,\epsilon)$$

and 
$$q(\tilde\theta|\theta^{(m)}, \epsilon) \sim N(\theta^{(m)},\epsilon)$$


Since they have the same proposals, the $q(\theta^{(m)}|\tilde\theta, \epsilon)$ and $q(\tilde\theta|\theta^{(m)}, \epsilon)$ will cancel each other out, rendering an $\alpha(\theta^{(m)},\tilde\theta)$ as

$$ \alpha(\theta^{(m)},\tilde\theta) = min(1,\frac{f(\tilde\theta|data)}{f(\theta^{(m)}|data)})= min(1,\frac{k(data|\tilde\theta)*f(\tilde\theta)}{k(data|\theta^{(m)})*f(\theta^{(m)})} )  $$



```{r, include=FALSE}
#K(x|theta)*f(theta) for model 1
K1 <- prod(dnorm(DATA[,1], log(1+exp(beta0M1 + beta1M1*DATA[,2])), sqrt(1/lamM1)))
fb0M1 <- dnorm(beta0M1, 0, 5)
fb1M1 <- dnorm(beta1M1, 0, 5)
flamM1 <- dgamma(lamM1, 0.5, 0.5)

numeratorM1 <- K1*fb0M1*fb1M1*flamM1

#K(x|theta)*f(theta) for model 2
K2 <- prod(dnorm(DATA[,1], log(1+exp(beta0M2 + beta1M2*DATA[,2] +beta2M2*DATA[,3])), sqrt(1/lamM2)))
fb0M2 <- dnorm(beta0M2, 0, 5)
fb1M2 <- dnorm(beta1M2, 0, 5)
fb2M2 <- dnorm(beta2M2, 0, 5)
flamM2 <- dgamma(lamM2, 0.5, 0.5)

numeratorM2 <- K2*fb0M2*fb1M2*fb2M2*flamM2
```

```{r, include=FALSE}
# MH step to figure out the denominator of each equation
f1 <- A1<- A1.1 <- Q1<- lammM1<- betam1M1<- betam0M1<- rep(0,burnin)
A1[1] <-A1.1[1] <- Q1[1] <- 0
m = t-burnin

for(i in 2:m){
    A1[i] = min(1,(prod(dnorm(DATA[,1], log(1+exp(beta01[i-1] + beta11[i-1]*DATA[,2])), sqrt(1/lam1[i-1])))*dnorm(beta01[i-1], 0, 5)*dnorm(beta11[i-1],0,5)*dgamma(lam1[i-1],0.5,0.5))/(numeratorM1)) 
    A1.1[i] = min(1, (numeratorM1)/(prod(dnorm(DATA[,1],log(1+exp(beta01[i-1] + beta11[i-1]*DATA[,2])), sqrt(1/lam1[i-1]))) * dnorm(beta01[i-1],0,5)*dnorm(beta11[i-1],0,5)*dgamma(lam1[i-1],0.5,0.5)))
    Q1[i] = dnorm(beta0M1,beta01[i-1],5)*dnorm(beta0M1, beta11[i-1],6)* dnorm(lamM1,lam1[i-1],10)
}

NUM1 = mean(A1.1*Q1)
DENOM1 = mean(A1)

denominatorM1 <- NUM1/DENOM1
kM1 <- numeratorM1/denominatorM1
```

```{r,include=FALSE}
f2 <- A2 <- A2.1<- Q2<- lammM2<- betam1M2<- betam0M2<- rep(0,burnin)
A2[1] <- A2.1[1] <- Q2[1] <- 0

for(i in 2:m){
    A2[i] = min(1, (prod(dnorm(DATA[,1], log(1+exp(beta02[i-1] + beta12[i-1]*DATA[,2] + beta22[i-1]*DATA[,3])), sqrt(1/lam2[i-1]))) * dnorm(beta02[i-1], 0, 5)*dnorm(beta12[i-1],0,5)*dnorm(beta22[i-1],0,5)*dgamma(lam2[i-1],0.5,0.5))/ (numeratorM2))
    A2.1[i] = min(1, (numeratorM2)/(prod(dnorm(DATA[,1], log(1+exp(beta02[i-1] + beta12[i-1]*DATA[,2] + beta22[i-1]*DATA[,3])), sqrt(1/lam2[i-1]))) * dnorm(beta02[i-1], 0, 5)* dnorm(beta12[i-1], 0, 5)*dnorm(beta22[i-1], 0, 5) * dgamma(lam2[i-1], 0.5, 0.5)))
    Q2[i] = dnorm(beta0M2,beta02[i-1],5)*dnorm(beta1M2,beta12[i-1],6)*dnorm(beta2M2,beta22[i-1],5)*dnorm(lamM2,lam2[i-1],10)
}

NUM2 = mean(A2.1*Q2)
DENOM2 = mean(A2)

denominatorM2 <- NUM2/DENOM2
kM2<- numeratorM2/denominatorM2
```

```{r, include=FALSE}
kM1/kM2
```





##Jumping Between Models


I will now try to estimate P(M=1|data) using a Metropolis jump between the two models. For a move from model 1 to model 2, the MH will follow the following steps: 

    sample u ~ U(0,1)
    if u < a(M1,M2)
      move to M2
      sample posteriors from M2
    else
      stay at M1
      sample posteriors from M1


This follows naturally for the move from model 2 to model 1, except that the $\alpha$ values are different. The samples of the posteriors are obtained by using a Gibbs framework for $\lambda$ and a Metropolis Hastings step for $(\beta_0, \beta_1, \beta_2)$. 


The $\alpha$ for moving from model 1 to model 2 will be
$$ \alpha_{12} = min(1, \frac{f(M=2, \beta_0, \beta_1, \beta_2, \lambda |data)}{f(M=1, \beta_0, \beta_1, \beta_2, \lambda|data} )$$


$$ \alpha_{12} = min(1, \frac{P(M=2)f(\lambda|M=2)f(\beta_0|M=2)f(\beta_1|M=2)f(\beta_2|M=2)\prod_ik(y_i|\beta_0,\beta_1,\beta_2,\lambda, M=2)} {P(M=1)f(\lambda|M=1)f(\beta_0|M=1)f(\beta_1|M=1)\prod_ik(y_i|\beta_0,\beta_1,\lambda, M=1)})$$


Many of the prior densities will cancel out, because they are the same for both model 1 and model 2. This leaves us with the following equation. 


$$ \alpha_{12} = min(1, \frac{f(\beta_2|M=2)\prod_ik(y_i|\beta_0,\beta_1,\beta_2,\lambda, M=2)} {f(\beta_2|M=1)\prod_ik(y_i|\beta_0,\beta_1,\lambda, M=1)})$$



The $\alpha$ for moving from model 2 to model 1 will be
$$ \alpha_{21} = min(1, \frac{f(M=1, \beta_0, \beta_1, \beta_2, \lambda |data)}{f(M=2, \beta_0, \beta_1, \beta_2, \lambda|data} )$$


$$ \alpha_{21} = min(1, \frac{P(M=1)f(\lambda|M=1)f(\beta_0|M=1)f(\beta_1|M=1)\prod_ik(y_i|\beta_0,\beta_1,\lambda, M=1)}{P(M=2)f(\lambda|M=2)f(\beta_0|M=2)f(\beta_1|M=2)f(\beta_2|M=2)\prod_ik(y_i|\beta_0,\beta_1,\beta_2,\lambda, M=2)} )$$

$$ \alpha_{21} = min(1, \frac{f(\beta_2|M=1)\prod_ik(y_i|\beta_0,\beta_1,\lambda, M=1)} {f(\beta_2|M=2)\prod_ik(y_i|\beta_0,\beta_1,\beta_2,\lambda, M=2)} )$$



I will take $f(\beta_2|M=1)$ to be a normal centered around 0, with a variance of 1, since we cannot make too many assumptions as to what the value of $\beta_2$ is from $\beta_0$ and $\beta_1$ from model 1. 




```{r, include=FALSE}
M <- b0 <- b1 <- b2 <- LAM <- y <- rep(0,t)
M[1] <- b0[1] <- b1[1] <- b2[1] <- LAM[1] <-1

for(i in 2:t){
    currentx = M[i-1]
    proposedx = (M[i-1] %% 2) + 1 
    
      n12 <- (prod(dnorm(DATA[,1], log(1+exp(b02[i-1] + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])), sqrt(1/LAM2[i-1]))) *dnorm(b22[i-1],0,5))
    
      d12 <- (prod(dnorm(DATA[,1], log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])), sqrt(1/LAM1[i-1])))*rnorm(1,0,1))
      
      A12 = n12/d12
      
      A21 = d12/n12
      
  if(currentx < proposedx){
      
      if(runif(1)<A12){
            M[i] = proposedx 
            
            #sample from model 2
            LAM2[i] <- rgamma(1, 0.5+25, 0.5+0.5*sum((DATA[,1]-log(1+exp(b02[i-1]+b12[i-1]*DATA[,2]+b22[i-1]*DATA[,3])))^2))
  
            #for b0 in model 2
            currentB02 = b02[i-1]
            proposedB02 = rnorm(1, mean=currentB02, sd=2) 
    
            NUMb02 <- ((exp(-0.5*((proposedB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(proposedB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB02, mean=proposedB02, sd=0.7)) 
    
            DENOMb02 <- ((exp(-0.5*((currentB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(currentB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) * dnorm(proposedB02, mean=currentB02, sd=0.7))
    
            NDb02 <- NUMb02/DENOMb02
    
            A = min(1,NDb02)
            if(runif(1)<A){
              b02[i] = proposedB02       # accept move with probabily min(1,A)
            }else{
              b02[i] = currentB02      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 2
            currentB12 = b12[i-1]
            proposedB12 = rnorm(1,mean=currentB12, sd=2) 
    
            NUMb12 <- ((exp(-0.5*((proposedB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + proposedB12*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB12, mean=proposedB12, sd=0.7)) 
    
            DENOMb12 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + currentB12*DATA[,2] + b22[i-1]*DATA[,3])))^2)))*dnorm(proposedB12, mean=currentB12, sd=0.7))
    
            NDb12 <- NUMb12/DENOMb12
    
            A = min(1, NDb12)
            if(runif(1)<A){
              b12[i] = proposedB12       # accept move with probabily min(1,A)
            }else{
              b12[i] = currentB12     # otherwise "reject" move, and stay where we are
            }
    
            #for b2 in model 2
            currentB22 = b22[i-1]
            proposedB22 = rnorm(1,mean=currentB22, sd=2)
    
            NUMb22 <- ((exp(-0.5*((proposedB22^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + proposedB22*DATA[,3])))^2))) *dnorm(currentB22, mean=proposedB22, sd=0.7)) 
    
            DENOMb22 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + currentB22*DATA[,3])))^2)))*dnorm(proposedB22, mean=currentB22, sd=0.7))
    
            NDb22 <- NUMb22/DENOMb22
    
            A = min(1, NDb22)
            if(runif(1)<A){
              b22[i] = proposedB22       # accept move with probabily min(1,A)
            }else{
              b22[i] = currentB22      # otherwise "reject" move, and stay where we are
            }
            
            y[i] = rnorm(1, log(1+exp(b02[i] + b12[i]*DATA[,2] + b22[i]*DATA[,3])), sqrt(1/LAM2[i])) 

      }else{
            M[i] = currentx 
            
            #sample from model 1
            LAM1[i] <- rgamma(1, shape = 0.5+ 25,  rate = 0.5 + 0.5*sum((DATA[,1] - log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])))^2))
    
            #for b0 in model 1
            currentB01 = b01[i-1]
            proposedB01 = rnorm(1, mean=currentB01, sd=2) 
    
            NUMb01 <- ((exp(-0.5*((proposedB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(proposedB01 + b11[i-1]*DATA[,2])))^2))) *dnorm(currentB01, mean=proposedB01, sd=2)) 
    
            DENOMb01 <- ((exp(-0.5*((currentB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1]-log(1 +exp(currentB01 + b11[i-1]*DATA[,2])))^2)))*dnorm(proposedB01, mean=currentB01, sd=2))
    
            NDb01 <- NUMb01/DENOMb01
    
            A = min(1, NDb01)
            if(runif(1)<A){
              b01[i] = proposedB01       # accept move with probabily min(1,A)
            }else{
              b01[i] = currentB01      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 1
            currentB11 = b11[i-1]
            proposedB11 = rnorm(1,mean=currentB11, sd=2) 
    
            NUMb11 <- ((exp(-0.5*((proposedB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + proposedB11*DATA[,2])))^2))) *dnorm(currentB11, mean=proposedB11, sd=2))
    
            DENOMb11 <- ((exp(-0.5*((currentB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + currentB11*DATA[,2])))^2))) *dnorm(proposedB11, mean=currentB11, sd=2))
    
            NDb11 <- NUMb11/DENOMb11
            A = min(1, NDb11)
            if(runif(1)<A){
              b11[i] = proposedB11       # accept move with probabily min(1,A)
            }else{
              b11[i] = currentB11     # otherwise "reject" move, and stay where we are
            }
            
            y[i] = rnorm(1, log(1+exp(b01[i] + b11[i]*DATA[,2])), sqrt(1/LAM1[i])) 
      }
            
    }else{
      
        if(runif(1) < A21){
            M[i] = proposedx 
            
            #sample from model 1
            
            LAM1[i] <- rgamma(1, shape = 0.5+ 25,  rate = 0.5 + 0.5*sum((DATA[,1] - log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])))^2))
    
            #for b0 in model 1
            currentB01 = b01[i-1]
            proposedB01 = rnorm(1, mean=currentB01, sd=2) 
    
            NUMb01 <- ((exp(-0.5*((proposedB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(proposedB01 + b11[i-1]*DATA[,2])))^2))) *dnorm(currentB01, mean=proposedB01, sd=2)) 
    
            DENOMb01 <- ((exp(-0.5*((currentB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1]-log(1 +exp(currentB01 + b11[i-1]*DATA[,2])))^2)))*dnorm(proposedB01, mean=currentB01, sd=2))
    
            NDb01 <- NUMb01/DENOMb01
    
            A = min(1, NDb01)
            if(runif(1)<A){
              b01[i] = proposedB01       # accept move with probabily min(1,A)
            }else{
              b01[i] = currentB01      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 1
            currentB11 = b11[i-1]
            proposedB11 = rnorm(1,mean=currentB11, sd=2) 
    
            NUMb11 <- ((exp(-0.5*((proposedB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + proposedB11*DATA[,2])))^2))) *dnorm(currentB11, mean=proposedB11, sd=2))
    
            DENOMb11 <- ((exp(-0.5*((currentB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + currentB11*DATA[,2])))^2))) *dnorm(proposedB11, mean=currentB11, sd=2))
    
            NDb11 <- NUMb11/DENOMb11
            A = min(1, NDb11)
            if(runif(1)<A){
              b11[i] = proposedB11       # accept move with probabily min(1,A)
            }else{
              b11[i] = currentB11     # otherwise "reject" move, and stay where we are
            }
            
            y[i] = rnorm(1, log(1+exp(b01[i] + b11[i]*DATA[,2])), sqrt(1/LAM1[i])) 
            
            
        }else{
            M[i] = currentx  
            
            #sample from model 2
            LAM2[i] <- rgamma(1, 0.5 + 25, 0.5+0.5*sum((DATA[,1]-log(1+exp(b02[i-1] + b12[i-1]*DATA[,2]+ b22[i-1]*DATA[,3])))^2))
  
  #for b0 in model 2
            currentB02 = b02[i-1]
            proposedB02 = rnorm(1, mean=currentB02, sd=2) 
    
            NUMb02 <- ((exp(-0.5*((proposedB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(proposedB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB02, mean=proposedB02, sd=0.7)) 
    
            DENOMb02 <- ((exp(-0.5*((currentB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(currentB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) * dnorm(proposedB02, mean=currentB02, sd=0.7))
    
            NDb02 <- NUMb02/DENOMb02
    
            A = min(1,NDb02)
            if(runif(1)<A){
              b02[i] = proposedB02       # accept move with probabily min(1,A)
            }else{
              b02[i] = currentB02      # otherwise "reject" move, and stay where we are
            }
    
    #for b1 in model 2
            currentB12 = b12[i-1]
            proposedB12 = rnorm(1,mean=currentB12, sd=2) 
    
            NUMb12 <- ((exp(-0.5*((proposedB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + proposedB12*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB12, mean=proposedB12, sd=0.7)) 
    
            DENOMb12 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + currentB12*DATA[,2] + b22[i-1]*DATA[,3])))^2)))*dnorm(proposedB12, mean=currentB12, sd=0.7))
    
            NDb12 <- NUMb12/DENOMb12
    
            A = min(1, NDb12)
            if(runif(1)<A){
              b12[i] = proposedB12       # accept move with probabily min(1,A)
            }else{
              b12[i] = currentB12     # otherwise "reject" move, and stay where we are
            }
    
            #for b2 in model 2
            currentB22 = b22[i-1]
            proposedB22 = rnorm(1,mean=currentB22, sd=2)
    
            NUMb22 <- ((exp(-0.5*((proposedB22^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + proposedB22*DATA[,3])))^2))) *dnorm(currentB22, mean=proposedB22, sd=0.7)) 
    
            DENOMb22 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + currentB22*DATA[,3])))^2)))*dnorm(proposedB22, mean=currentB22, sd=0.7))
    
            NDb22 <- NUMb22/DENOMb22
    
            A = min(1, NDb22)
            if(runif(1)<A){
              b22[i] = proposedB22       # accept move with probabily min(1,A)
            }else{
              b22[i] = currentB22      # otherwise "reject" move, and stay where we are
            }
            
            y[i] = rnorm(1, log(1+exp(b02[i] + b12[i]*DATA[,2] + b22[i]*DATA[,3])), sqrt(1/LAM2[i])) 
            
        }
    }
  }
  

```

```{r, include=FALSE}
1-mean(M-1)
```

```{r, echo=FALSE}
par(mfrow= c(2,2))
plot(y, type='l')
hist(y,100)
plot(lam1, type='l')
hist(lam1,100)
plot(beta01, type = 'l')
hist(beta01,100)
plot(beta11, type = 'l')
hist(beta11,100)
plot(lam2, type='l')
hist(lam2,100)
plot(beta02, type = 'l')
hist(beta02,100)
plot(beta12, type = 'l')
hist(beta12,100)
plot(beta22, type = 'l')
hist(beta22,100)
```



##Results


The Bayes Factor calculation renders a ratio of 3.025. Recall that this is equal to $\frac{P(M1|y_i,x_{1i},x_{2i})} {P(M2|y_i,x_{1i},x_{2i})} =\frac{P(M1|y_i,x_{1i},x_{2i})} {1-P(M1|y_i,x_{1i},x_{2i})}$. This means that $P(M1|y_i,x_{1i},x_{2i}) = 0.752$ and $P(M2|y_i,x_{1i},x_{2i}) = 0.248$. In lay-mans terms, the chain is estimated to spend approximately 75\% of its time taking samples from the first model, and about 25\% of its time taking samples from the larger, second model. 


The results of the jumping metropolis reveals that the estimated $P(M1|y_i,x_{1i},x_{2i}) = 0.749$ and $P(M2|y_i,x_{1i},x_{2i}) = 0.251$. These values were found by calculating the percentage of samples that were taken from model 1, over all iterations of the chain in the jumping metropolis framework. These are very similar results to the Bayes Factor, which we should expect. 


These findings confirm that the Monte Carlo Markov chain spends approximately 75\% of its time sampling from model 1 and 25\% of its time sampling from model 2. This may reveal that majority of the observations in our data are coming from a population that is best represented by model 1. 





##Estimating y51


The final portion of this assignment is to determine the predictive density for $y_{51}$, for an observation with $x_{1,51} = 0.23$ and $x_{2,51} = -1.2$ using a Monte Carlo Markov chain method. I looped through the chain using a jumping metropolis step between model 1 and model 1, and for each sampled value, I also sampled $y_{51}$ from $N(y_{i,51}| log(1+exp(\beta_0 + \beta_1*0.23)), \sqrt{\frac{1}{\lambda}})$ if the samples came from model 1, and $N(y_{i,51}| log(1+exp(\beta_0 + \beta_1*0.23 - \beta_2*1.2)), \sqrt{\frac{1}{\lambda}})$ if the samples came from model 2. I collected 100,000 observations with 20,000 burnin of $y_{51}$, and found the mean to be 0.1000257. 

Below is a trace plot and histogram of the predictive density of $y_{51}$. We see that the chain moves well, and the predictive density is almost symmetrically distributed around the mean of 0.1000257. 


```{r,include=FALSE}
M <- b0 <- b1 <- b2 <- LAM <- y51 <- rep(0,t)
M[1] <- b0[1] <- b1[1] <- b2[1] <- LAM[1] <-1

for(i in 2:t){
    currentx = M[i-1]
    proposedx = (M[i-1] %% 2) + 1 
    
      n12 <- (prod(dnorm(DATA[,1], log(1+exp(b02[i-1] + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])), sqrt(1/LAM2[i-1])))* dnorm(b22[i-1],0,5))
    
      d12 <- (prod(dnorm(DATA[,1], log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])), sqrt(1/LAM1[i-1])))*rnorm(1,0,1))
      
      A12 = n12/d12
      
      A21 = d12/n12
      
  if(currentx < proposedx){
      
      if(runif(1)<A12){
            M[i] = proposedx 
            
            #sample from model 2
            LAM2[i] <- rgamma(1, 0.5+25, 0.5+0.5*sum((DATA[,1]-log(1+exp(b02[i-1]+b12[i-1]*DATA[,2]+b22[i-1]*DATA[,3])))^2))
  
            #for b0 in model 2
            currentB02 = b02[i-1]
            proposedB02 = rnorm(1, mean=currentB02, sd=2) 
    
            NUMb02 <- ((exp(-0.5*((proposedB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(proposedB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB02, mean=proposedB02, sd=0.7)) 
    
            DENOMb02 <- ((exp(-0.5*((currentB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(currentB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) * dnorm(proposedB02, mean=currentB02, sd=0.7))
    
            NDb02 <- NUMb02/DENOMb02
    
            A = min(1,NDb02)
            if(runif(1)<A){
              b02[i] = proposedB02       # accept move with probabily min(1,A)
            }else{
              b02[i] = currentB02      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 2
            currentB12 = b12[i-1]
            proposedB12 = rnorm(1,mean=currentB12, sd=2) 
    
            NUMb12 <- ((exp(-0.5*((proposedB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + proposedB12*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB12, mean=proposedB12, sd=0.7)) 
    
            DENOMb12 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + currentB12*DATA[,2] + b22[i-1]*DATA[,3])))^2)))*dnorm(proposedB12, mean=currentB12, sd=0.7))
    
            NDb12 <- NUMb12/DENOMb12
    
            A = min(1, NDb12)
            if(runif(1)<A){
              b12[i] = proposedB12       # accept move with probabily min(1,A)
            }else{
              b12[i] = currentB12     # otherwise "reject" move, and stay where we are
            }
    
            #for b2 in model 2
            currentB22 = b22[i-1]
            proposedB22 = rnorm(1,mean=currentB22, sd=2)
    
            NUMb22 <- ((exp(-0.5*((proposedB22^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + proposedB22*DATA[,3])))^2))) *dnorm(currentB22, mean=proposedB22, sd=0.7)) 
    
            DENOMb22 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + currentB22*DATA[,3])))^2)))*dnorm(proposedB22, mean=currentB22, sd=0.7))
    
            NDb22 <- NUMb22/DENOMb22
    
            A = min(1, NDb22)
            if(runif(1)<A){
              b22[i] = proposedB22       # accept move with probabily min(1,A)
            }else{
              b22[i] = currentB22      # otherwise "reject" move, and stay where we are
            }
            y51[i] = rnorm(1, log(1+exp(b02[i] + b12[i]*0.23 - b22[i]*1.2)), sqrt(1/LAM2[i]))

      }else{
            M[i] = currentx 
            
            #sample from model 1
            LAM1[i] <- rgamma(1, shape = 0.5+ 25,  rate = 0.5 + 0.5*sum((DATA[,1] - log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])))^2))
    
            #for b0 in model 1
            currentB01 = b01[i-1]
            proposedB01 = rnorm(1, mean=currentB01, sd=2) 
    
            NUMb01 <- ((exp(-0.5*((proposedB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(proposedB01 + b11[i-1]*DATA[,2])))^2))) *dnorm(currentB01, mean=proposedB01, sd=2)) 
    
            DENOMb01 <- ((exp(-0.5*((currentB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1]-log(1 +exp(currentB01 + b11[i-1]*DATA[,2])))^2)))*dnorm(proposedB01, mean=currentB01, sd=2))
    
            NDb01 <- NUMb01/DENOMb01
    
            A = min(1, NDb01)
            if(runif(1)<A){
              b01[i] = proposedB01       # accept move with probabily min(1,A)
            }else{
              b01[i] = currentB01      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 1
            currentB11 = b11[i-1]
            proposedB11 = rnorm(1,mean=currentB11, sd=2) 
    
            NUMb11 <- ((exp(-0.5*((proposedB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + proposedB11*DATA[,2])))^2))) *dnorm(currentB11, mean=proposedB11, sd=2))
    
            DENOMb11 <- ((exp(-0.5*((currentB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + currentB11*DATA[,2])))^2))) *dnorm(proposedB11, mean=currentB11, sd=2))
    
            NDb11 <- NUMb11/DENOMb11
            A = min(1, NDb11)
            if(runif(1)<A){
              b11[i] = proposedB11       # accept move with probabily min(1,A)
            }else{
              b11[i] = currentB11     # otherwise "reject" move, and stay where we are
            }
            y51[i] = rnorm(1, log(1+exp(b01[i] + b11[i]*0.23)), sqrt(1/LAM1[i])) 
      }
            
    }else{
      
        if(runif(1) < A21){
            M[i] = proposedx 
            
            #sample from model 1
            
            LAM1[i] <- rgamma(1, shape = 0.5+ 25,  rate = 0.5 + 0.5*sum((DATA[,1] - log(1+exp(b01[i-1] + b11[i-1]*DATA[,2])))^2))
    
            #for b0 in model 1
            currentB01 = b01[i-1]
            proposedB01 = rnorm(1, mean=currentB01, sd=2) 
    
            NUMb01 <- ((exp(-0.5*((proposedB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(proposedB01 + b11[i-1]*DATA[,2])))^2))) *dnorm(currentB01, mean=proposedB01, sd=2)) 
    
            DENOMb01 <- ((exp(-0.5*((currentB01^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1]-log(1 +exp(currentB01 + b11[i-1]*DATA[,2])))^2)))*dnorm(proposedB01, mean=currentB01, sd=2))
    
            NDb01 <- NUMb01/DENOMb01
    
            A = min(1, NDb01)
            if(runif(1)<A){
              b01[i] = proposedB01       # accept move with probabily min(1,A)
            }else{
              b01[i] = currentB01      # otherwise "reject" move, and stay where we are
            }
    
            #for b1 in model 1
            currentB11 = b11[i-1]
            proposedB11 = rnorm(1,mean=currentB11, sd=2) 
    
            NUMb11 <- ((exp(-0.5*((proposedB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + proposedB11*DATA[,2])))^2))) *dnorm(currentB11, mean=proposedB11, sd=2))
    
            DENOMb11 <- ((exp(-0.5*((currentB11^2)*0.04) - 0.5*LAM1[i-1]*sum((DATA[,1] - log(1 +exp(b01[i-1] + currentB11*DATA[,2])))^2))) *dnorm(proposedB11, mean=currentB11, sd=2))
    
            NDb11 <- NUMb11/DENOMb11
            A = min(1, NDb11)
            if(runif(1)<A){
              b11[i] = proposedB11       # accept move with probabily min(1,A)
            }else{
              b11[i] = currentB11     # otherwise "reject" move, and stay where we are
            }
            y51[i] = rnorm(1, log(1+exp(b01[i] + b11[i]*0.23)), sqrt(1/LAM1[i])) 
            
            
        }else{
            M[i] = currentx  
            
            #sample from model 2
            LAM2[i] <- rgamma(1, 0.5 + 25, 0.5+0.5*sum((DATA[,1]-log(1+exp(b02[i-1] + b12[i-1]*DATA[,2]+ b22[i-1]*DATA[,3])))^2))
  
  #for b0 in model 2
            currentB02 = b02[i-1]
            proposedB02 = rnorm(1, mean=currentB02, sd=2) 
    
            NUMb02 <- ((exp(-0.5*((proposedB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(proposedB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB02, mean=proposedB02, sd=0.7)) 
    
            DENOMb02 <- ((exp(-0.5*((currentB02^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(currentB02 + b12[i-1]*DATA[,2] + b22[i-1]*DATA[,3])))^2))) * dnorm(proposedB02, mean=currentB02, sd=0.7))
    
            NDb02 <- NUMb02/DENOMb02
    
            A = min(1,NDb02)
            if(runif(1)<A){
              b02[i] = proposedB02       # accept move with probabily min(1,A)
            }else{
              b02[i] = currentB02      # otherwise "reject" move, and stay where we are
            }
    
    #for b1 in model 2
            currentB12 = b12[i-1]
            proposedB12 = rnorm(1,mean=currentB12, sd=2) 
    
            NUMb12 <- ((exp(-0.5*((proposedB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + proposedB12*DATA[,2] + b22[i-1]*DATA[,3])))^2))) *dnorm(currentB12, mean=proposedB12, sd=0.7)) 
    
            DENOMb12 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + currentB12*DATA[,2] + b22[i-1]*DATA[,3])))^2)))*dnorm(proposedB12, mean=currentB12, sd=0.7))
    
            NDb12 <- NUMb12/DENOMb12
    
            A = min(1, NDb12)
            if(runif(1)<A){
              b12[i] = proposedB12       # accept move with probabily min(1,A)
            }else{
              b12[i] = currentB12     # otherwise "reject" move, and stay where we are
            }
    
            #for b2 in model 2
            currentB22 = b22[i-1]
            proposedB22 = rnorm(1,mean=currentB22, sd=2)
    
            NUMb22 <- ((exp(-0.5*((proposedB22^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1] - log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + proposedB22*DATA[,3])))^2))) *dnorm(currentB22, mean=proposedB22, sd=0.7)) 
    
            DENOMb22 <- ((exp(-0.5*((currentB12^2)*0.04) - 0.5*LAM2[i-1]*sum((DATA[,1]-log(1 +exp(b02[i-1] + b12[i-1]*DATA[,2] + currentB22*DATA[,3])))^2)))*dnorm(proposedB22, mean=currentB22, sd=0.7))
    
            NDb22 <- NUMb22/DENOMb22
    
            A = min(1, NDb22)
            if(runif(1)<A){
              b22[i] = proposedB22       # accept move with probabily min(1,A)
            }else{
              b22[i] = currentB22      # otherwise "reject" move, and stay where we are
            }
            y51[i] = rnorm(1, log(1+exp(b02[i] + b12[i]*0.23 - b22[i]*1.2)), sqrt(1/LAM2[i]))
            
        }
    }
  }
```

```{r, include=FALSE}
y_51 <- y51[-(1:burnin)]
1-mean(M-1)
mean(y_51)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(y_51,type='l')
hist(y_51, 100)
```


















